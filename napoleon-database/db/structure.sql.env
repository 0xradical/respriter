BEGIN;

-- Extracted and modified from: github.com/gavinwahl/postgres-json-schema @postgres-json-schema--0.1.1.sql

CREATE SCHEMA IF NOT EXISTS json_schema;

CREATE OR REPLACE FUNCTION json_schema._validate_json_schema_type(type text, data jsonb) RETURNS boolean AS $$
BEGIN
  IF type = 'integer' THEN
    IF jsonb_typeof(data) != 'number' THEN
      RETURN false;
    END IF;
    IF trunc(data::text::numeric) != data::text::numeric THEN
      RETURN false;
    END IF;
  ELSE
    IF type != jsonb_typeof(data) THEN
      RETURN false;
    END IF;
  END IF;
  RETURN true;
END;
$$ LANGUAGE 'plpgsql' IMMUTABLE;


CREATE OR REPLACE FUNCTION json_schema.validate_json_schema(schema jsonb, data jsonb, root_schema jsonb DEFAULT NULL) RETURNS boolean AS $$
DECLARE
  prop text;
  item jsonb;
  path text[];
  types text[];
  pattern text;
  props text[];
BEGIN
  IF root_schema IS NULL THEN
    root_schema = schema;
  END IF;

  IF schema ? 'type' THEN
    IF jsonb_typeof(schema->'type') = 'array' THEN
      types = ARRAY(SELECT jsonb_array_elements_text(schema->'type'));
    ELSE
      types = ARRAY[schema->>'type'];
    END IF;
    IF (SELECT NOT bool_or(json_schema._validate_json_schema_type(type, data)) FROM unnest(types) type) THEN
      RETURN false;
    END IF;
  END IF;

  IF schema ? 'properties' THEN
    FOR prop IN SELECT jsonb_object_keys(schema->'properties') LOOP
      IF data ? prop AND NOT json_schema.validate_json_schema(schema->'properties'->prop, data->prop, root_schema) THEN
        RETURN false;
      END IF;
    END LOOP;
  END IF;

  IF schema ? 'required' AND jsonb_typeof(data) = 'object' THEN
    IF NOT ARRAY(SELECT jsonb_object_keys(data)) @>
           ARRAY(SELECT jsonb_array_elements_text(schema->'required')) THEN
      RETURN false;
    END IF;
  END IF;

  IF schema ? 'items' AND jsonb_typeof(data) = 'array' THEN
    IF jsonb_typeof(schema->'items') = 'object' THEN
      FOR item IN SELECT jsonb_array_elements(data) LOOP
        IF NOT json_schema.validate_json_schema(schema->'items', item, root_schema) THEN
          RETURN false;
        END IF;
      END LOOP;
    ELSE
      IF NOT (
        SELECT bool_and(i > jsonb_array_length(schema->'items') OR json_schema.validate_json_schema(schema->'items'->(i::int - 1), elem, root_schema))
        FROM jsonb_array_elements(data) WITH ORDINALITY AS t(elem, i)
      ) THEN
        RETURN false;
      END IF;
    END IF;
  END IF;

  IF jsonb_typeof(schema->'additionalItems') = 'boolean' and NOT (schema->'additionalItems')::text::boolean AND jsonb_typeof(schema->'items') = 'array' THEN
    IF jsonb_array_length(data) > jsonb_array_length(schema->'items') THEN
      RETURN false;
    END IF;
  END IF;

  IF jsonb_typeof(schema->'additionalItems') = 'object' THEN
    IF NOT (
        SELECT bool_and(json_schema.validate_json_schema(schema->'additionalItems', elem, root_schema))
        FROM jsonb_array_elements(data) WITH ORDINALITY AS t(elem, i)
        WHERE i > jsonb_array_length(schema->'items')
      ) THEN
      RETURN false;
    END IF;
  END IF;

  IF schema ? 'minimum' AND jsonb_typeof(data) = 'number' THEN
    IF data::text::numeric < (schema->>'minimum')::numeric THEN
      RETURN false;
    END IF;
  END IF;

  IF schema ? 'maximum' AND jsonb_typeof(data) = 'number' THEN
    IF data::text::numeric > (schema->>'maximum')::numeric THEN
      RETURN false;
    END IF;
  END IF;

  IF COALESCE((schema->'exclusiveMinimum')::text::bool, FALSE) THEN
    IF data::text::numeric = (schema->>'minimum')::numeric THEN
      RETURN false;
    END IF;
  END IF;

  IF COALESCE((schema->'exclusiveMaximum')::text::bool, FALSE) THEN
    IF data::text::numeric = (schema->>'maximum')::numeric THEN
      RETURN false;
    END IF;
  END IF;

  IF schema ? 'anyOf' THEN
    IF NOT (SELECT bool_or(json_schema.validate_json_schema(sub_schema, data, root_schema)) FROM jsonb_array_elements(schema->'anyOf') sub_schema) THEN
      RETURN false;
    END IF;
  END IF;

  IF schema ? 'allOf' THEN
    IF NOT (SELECT bool_and(json_schema.validate_json_schema(sub_schema, data, root_schema)) FROM jsonb_array_elements(schema->'allOf') sub_schema) THEN
      RETURN false;
    END IF;
  END IF;

  IF schema ? 'oneOf' THEN
    IF 1 != (SELECT COUNT(*) FROM jsonb_array_elements(schema->'oneOf') sub_schema WHERE json_schema.validate_json_schema(sub_schema, data, root_schema)) THEN
      RETURN false;
    END IF;
  END IF;

  IF COALESCE((schema->'uniqueItems')::text::boolean, false) THEN
    IF (SELECT COUNT(*) FROM jsonb_array_elements(data)) != (SELECT count(DISTINCT val) FROM jsonb_array_elements(data) val) THEN
      RETURN false;
    END IF;
  END IF;

  IF schema ? 'additionalProperties' AND jsonb_typeof(data) = 'object' THEN
    props := ARRAY(
      SELECT key
      FROM jsonb_object_keys(data) key
      WHERE key NOT IN (SELECT jsonb_object_keys(schema->'properties'))
        AND NOT EXISTS (SELECT * FROM jsonb_object_keys(schema->'patternProperties') pat WHERE key ~ pat)
    );
    IF jsonb_typeof(schema->'additionalProperties') = 'boolean' THEN
      IF NOT (schema->'additionalProperties')::text::boolean AND jsonb_typeof(data) = 'object' AND NOT props <@ ARRAY(SELECT jsonb_object_keys(schema->'properties')) THEN
        RETURN false;
      END IF;
    ELSEIF NOT (
      SELECT bool_and(json_schema.validate_json_schema(schema->'additionalProperties', data->key, root_schema))
      FROM unnest(props) key
    ) THEN
      RETURN false;
    END IF;
  END IF;

  IF schema ? '$ref' THEN
    path := ARRAY(
      SELECT regexp_replace(regexp_replace(path_part, '~1', '/'), '~0', '~')
      FROM UNNEST(regexp_split_to_array(schema->>'$ref', '/')) path_part
    );
    -- ASSERT path[1] = '#', 'only refs anchored at the root are supported';
    IF NOT json_schema.validate_json_schema(root_schema #> path[2:array_length(path, 1)], data, root_schema) THEN
      RETURN false;
    END IF;
  END IF;

  IF schema ? 'enum' THEN
    IF NOT EXISTS (SELECT * FROM jsonb_array_elements(schema->'enum') val WHERE val = data) THEN
      RETURN false;
    END IF;
  END IF;

  IF schema ? 'minLength' AND jsonb_typeof(data) = 'string' THEN
    IF char_length(data #>> '{}') < (schema->>'minLength')::numeric THEN
      RETURN false;
    END IF;
  END IF;

  IF schema ? 'maxLength' AND jsonb_typeof(data) = 'string' THEN
    IF char_length(data #>> '{}') > (schema->>'maxLength')::numeric THEN
      RETURN false;
    END IF;
  END IF;

  IF schema ? 'not' THEN
    IF json_schema.validate_json_schema(schema->'not', data, root_schema) THEN
      RETURN false;
    END IF;
  END IF;

  IF schema ? 'maxProperties' AND jsonb_typeof(data) = 'object' THEN
    IF (SELECT count(*) FROM jsonb_object_keys(data)) > (schema->>'maxProperties')::numeric THEN
      RETURN false;
    END IF;
  END IF;

  IF schema ? 'minProperties' AND jsonb_typeof(data) = 'object' THEN
    IF (SELECT count(*) FROM jsonb_object_keys(data)) < (schema->>'minProperties')::numeric THEN
      RETURN false;
    END IF;
  END IF;

  IF schema ? 'maxItems' AND jsonb_typeof(data) = 'array' THEN
    IF (SELECT count(*) FROM jsonb_array_elements(data)) > (schema->>'maxItems')::numeric THEN
      RETURN false;
    END IF;
  END IF;

  IF schema ? 'minItems' AND jsonb_typeof(data) = 'array' THEN
    IF (SELECT count(*) FROM jsonb_array_elements(data)) < (schema->>'minItems')::numeric THEN
      RETURN false;
    END IF;
  END IF;

  IF schema ? 'dependencies' THEN
    FOR prop IN SELECT jsonb_object_keys(schema->'dependencies') LOOP
      IF data ? prop THEN
        IF jsonb_typeof(schema->'dependencies'->prop) = 'array' THEN
          IF NOT (SELECT bool_and(data ? dep) FROM jsonb_array_elements_text(schema->'dependencies'->prop) dep) THEN
            RETURN false;
          END IF;
        ELSE
          IF NOT json_schema.validate_json_schema(schema->'dependencies'->prop, data, root_schema) THEN
            RETURN false;
          END IF;
        END IF;
      END IF;
    END LOOP;
  END IF;

  IF schema ? 'pattern' AND jsonb_typeof(data) = 'string' THEN
    IF (data #>> '{}') !~ (schema->>'pattern') THEN
      RETURN false;
    END IF;
  END IF;

  IF schema ? 'patternProperties' AND jsonb_typeof(data) = 'object' THEN
    FOR prop IN SELECT jsonb_object_keys(data) LOOP
      FOR pattern IN SELECT jsonb_object_keys(schema->'patternProperties') LOOP
        RAISE NOTICE 'prop %s, pattern %, schema %', prop, pattern, schema->'patternProperties'->pattern;
        IF prop ~ pattern AND NOT json_schema.validate_json_schema(schema->'patternProperties'->pattern, data->prop, root_schema) THEN
          RETURN false;
        END IF;
      END LOOP;
    END LOOP;
  END IF;

  IF schema ? 'multipleOf' AND jsonb_typeof(data) = 'number' THEN
    IF data::text::numeric % (schema->>'multipleOf')::numeric != 0 THEN
      RETURN false;
    END IF;
  END IF;

  RETURN true;
END;
$$ LANGUAGE 'plpgsql' IMMUTABLE;

CREATE SCHEMA IF NOT EXISTS api;
CREATE SCHEMA IF NOT EXISTS app;
CREATE SCHEMA IF NOT EXISTS jwt;
CREATE SCHEMA IF NOT EXISTS util;
CREATE SCHEMA IF NOT EXISTS triggers;
CREATE SCHEMA IF NOT EXISTS settings;

CREATE EXTENSION IF NOT EXISTS plpgsql     WITH SCHEMA pg_catalog;
CREATE EXTENSION IF NOT EXISTS pgcrypto    WITH SCHEMA public;
CREATE EXTENSION IF NOT EXISTS "uuid-ossp" WITH SCHEMA public;

CREATE TYPE app.pipe_process_status AS ENUM (
  'pending',
  'skipped',
  'waiting',
  'failed',
  'succeeded'
);

CREATE TYPE app.pipeline_status AS ENUM (
  'pending',
  'waiting',
  'succeeded',
  'failed'
);

CREATE TYPE app.resource_status AS ENUM (
  'active',
  'deleted',
  'expired'
);

CREATE TABLE app.datasets (
  id         uuid        DEFAULT public.uuid_generate_v1() PRIMARY KEY,
  sequence   bigint      DEFAULT 0,
  created_at timestamptz DEFAULT NOW()                     NOT NULL,
  updated_at timestamptz DEFAULT NOW()                     NOT NULL,
  name       varchar
);

CREATE TABLE app.pipeline_templates (
  id                    uuid        DEFAULT public.uuid_generate_v1() PRIMARY KEY,
  dataset_id            uuid        REFERENCES app.datasets(id)       ON DELETE CASCADE,
  created_at            timestamptz DEFAULT NOW()                     NOT NULL,
  updated_at            timestamptz DEFAULT NOW()                     NOT NULL,
  max_retries           integer     DEFAULT 5                         NOT NULL,
  name                  varchar                                       NOT NULL,
  data                  jsonb       DEFAULT '{}',
  pipes                 jsonb       DEFAULT '[]',
  bootstrap_script_type varchar,
  success_script_type   varchar,
  fail_script_type      varchar,
  waiting_script_type   varchar,
  bootstrap_script      text,
  success_script        text,
  fail_script           text,
  waiting_script        text
);

CREATE TABLE app.pipeline_executions (
  id                   uuid                DEFAULT public.uuid_generate_v1()     PRIMARY KEY,
  dataset_id           uuid                REFERENCES app.datasets(id)           ON DELETE CASCADE,
  status               app.pipeline_status DEFAULT 'pending',
  pipeline_template_id uuid                REFERENCES app.pipeline_templates(id) ON DELETE CASCADE,
  schedule_interval    interval,
  created_at           timestamptz         DEFAULT NOW()                         NOT NULL,
  updated_at           timestamptz         DEFAULT NOW()                         NOT NULL,
  run_at               timestamptz,
  counter              integer             DEFAULT 1                             NOT NULL,
  name                 varchar                                                   NOT NULL
);

CREATE TABLE app.pipelines (
  id                    uuid                DEFAULT public.uuid_generate_v1()      PRIMARY KEY,
  dataset_id            uuid                REFERENCES app.datasets(id)            ON DELETE CASCADE,
  pipeline_execution_id uuid                REFERENCES app.pipeline_executions(id) ON DELETE CASCADE,
  pipeline_template_id  uuid                REFERENCES app.pipeline_templates(id)  ON DELETE SET NULL,
  status                app.pipeline_status DEFAULT 'pending',
  total_count           integer             DEFAULT 0,
  waiting_count         integer             DEFAULT 0,
  failed_count          integer             DEFAULT 0,
  succeeded_count       integer             DEFAULT 0,
  created_at            timestamptz         DEFAULT NOW()                          NOT NULL,
  updated_at            timestamptz         DEFAULT NOW()                          NOT NULL,
  max_retries           integer,
  name                  varchar                                                    NOT NULL,
  data                  jsonb,
  pipes                 jsonb,
  bootstrap_script_type varchar,
  success_script_type   varchar,
  fail_script_type      varchar,
  waiting_script_type   varchar,
  bootstrap_script      text,
  success_script        text,
  fail_script           text,
  waiting_script        text
);

CREATE TABLE app.pipe_processes (
  id                  uuid                    DEFAULT public.uuid_generate_v1() PRIMARY KEY,
  pipeline_id         uuid                    REFERENCES app.pipelines(id)      ON DELETE CASCADE,
  status              app.pipe_process_status DEFAULT 'pending'                 NOT NULL,
  process_index       integer                 DEFAULT 0                         NOT NULL,
  created_at          timestamptz             DEFAULT NOW()                     NOT NULL,
  updated_at          timestamptz             DEFAULT NOW()                     NOT NULL,
  initial_accumulator jsonb,
  accumulator         jsonb,
  last_accumulator    jsonb,
  data                jsonb,
  last_data           jsonb,
  error_backtrace     varchar[],
  retried_in          integer[]
);

CREATE TABLE app.resource_schemas (
  id                    uuid        DEFAULT public.uuid_generate_v1() PRIMARY KEY,
  dataset_id            uuid        REFERENCES app.datasets(id)       ON DELETE CASCADE NOT NULL,
  created_at            timestamptz DEFAULT NOW()                                       NOT NULL,
  updated_at            timestamptz DEFAULT NOW()                                       NOT NULL,
  kind                  varchar                                                         NOT NULL,
  schema_version        varchar                                                         NOT NULL,
  specification         jsonb                                                           NOT NULL,
  public_specification  jsonb                                                           NOT NULL
);

CREATE TABLE app.resources (
  id                uuid                DEFAULT public.uuid_generate_v1() PRIMARY KEY,
  dataset_id        uuid                REFERENCES app.datasets(id)       ON DELETE CASCADE NOT NULL,
  sequence          bigint              DEFAULT 1                                           NOT NULL,
  status            app.resource_status DEFAULT 'active'                                    NOT NULL,
  created_at        timestamptz         DEFAULT NOW()                                       NOT NULL,
  updated_at        timestamptz         DEFAULT NOW()                                       NOT NULL,
  unique_id         varchar(40)                                                             NOT NULL,
  kind              varchar                                                                 NOT NULL,
  schema_version    varchar,
  content           jsonb               DEFAULT '{}',
  relations         jsonb               DEFAULT '{}',
  extra             jsonb,
  last_execution_id uuid
);

CREATE TABLE app.resource_versions (
  id                bigserial            PRIMARY KEY,
  resource_id       uuid                 REFERENCES app.resources(id) ON DELETE CASCADE,
  dataset_sequence  bigint,
  dataset_id        uuid                 REFERENCES app.datasets(id)  ON DELETE CASCADE,
  sequence          bigint,
  status            app.resource_status,
  created_at        timestamptz,
  updated_at        timestamptz,
  unique_id         varchar(40),
  kind              varchar,
  schema_version    varchar,
  content           jsonb,
  relations         jsonb,
  extra             jsonb,
  last_execution_id uuid
);

CREATE TABLE app.users (
  id         uuid         DEFAULT public.uuid_generate_v1() PRIMARY KEY,
  dataset_id uuid         REFERENCES app.datasets(id)       ON DELETE CASCADE,
  created_at timestamptz  DEFAULT NOW()                     NOT NULL,
  updated_at timestamptz  DEFAULT NOW()                     NOT NULL,
  password   varchar(60)  DEFAULT ''                        NOT NULL,
  username   varchar      DEFAULT ''                        NOT NULL
);

CREATE TABLE IF NOT EXISTS public.ar_internal_metadata (
  key        varchar     PRIMARY KEY,
  value      varchar,
  created_at timestamptz DEFAULT NOW() NOT NULL,
  updated_at timestamptz DEFAULT NOW() NOT NULL
);

CREATE FUNCTION public.que_validate_tags(tags_array jsonb) RETURNS boolean
    LANGUAGE sql
    AS $$
  SELECT bool_and(
    jsonb_typeof(value) = 'string'
    AND
    char_length(value::text) <= 100
  )
  FROM jsonb_array_elements(tags_array)
$$;

CREATE TABLE public.que_jobs (
  id                   bigserial    PRIMARY KEY,

  priority             smallint     DEFAULT 100 NOT NULL,
  run_at               timestamptz  DEFAULT NOW() NOT NULL,
  job_class            text         NOT NULL,
  error_count          integer      DEFAULT 0 NOT NULL,
  last_error_message   text,
  queue                text         DEFAULT 'default'::text NOT NULL,
  last_error_backtrace text,
  finished_at          timestamptz,
  expired_at           timestamptz,
  args                 jsonb        DEFAULT '[]'::jsonb NOT NULL,
  data                 jsonb        DEFAULT '{}'::jsonb NOT NULL,

  CONSTRAINT error_length
  CHECK (((char_length(last_error_message) <= 500) AND (char_length(last_error_backtrace) <= 10000))),

  CONSTRAINT job_class_length
  CHECK ((char_length(
CASE job_class
  WHEN 'ActiveJob::QueueAdapters::QueAdapter::JobWrapper'::text THEN ((args -> 0) ->> 'job_class'::text)
  ELSE job_class
END) <= 200)),

  CONSTRAINT queue_length
  CHECK ((char_length(queue) <= 100)),

  CONSTRAINT valid_args
  CHECK ((jsonb_typeof(args) = 'array'::text)),

  CONSTRAINT valid_data
  CHECK (((jsonb_typeof(data) = 'object'::text) AND ((NOT (data ? 'tags'::text)) OR ((jsonb_typeof((data -> 'tags'::text)) = 'array'::text) AND (jsonb_array_length((data -> 'tags'::text)) <= 5) AND public.que_validate_tags((data -> 'tags'::text))))))
)
WITH (fillfactor='90');

-- Required for migration? Maybe....
COMMENT ON TABLE public.que_jobs IS '4';

CREATE UNLOGGED TABLE public.que_lockers (
  pid               int     PRIMARY KEY,
  worker_count      int     NOT NULL,
  worker_priorities int[]   NOT NULL,
  ruby_pid          int     NOT NULL,
  ruby_hostname     text    NOT NULL,
  queues            text[]  NOT NULL,
  listening         boolean NOT NULL,

  CONSTRAINT valid_queues
  CHECK (((array_ndims(queues) = 1) AND (array_length(queues, 1) IS NOT NULL))),

  CONSTRAINT valid_worker_priorities
  CHECK (((array_ndims(worker_priorities) = 1) AND (array_length(worker_priorities, 1) IS NOT NULL)))
);

CREATE TABLE public.que_values (
  key   text  PRIMARY KEY,
  value jsonb DEFAULT '{}'::jsonb NOT NULL,

  CONSTRAINT valid_value
  CHECK ((jsonb_typeof(value) = 'object'::text))
)
WITH (fillfactor='90');

CREATE TABLE IF NOT EXISTS public.schema_migrations (
  version varchar PRIMARY KEY
);

CREATE TABLE settings.secrets (
  key   varchar PRIMARY KEY,
  value varchar
);

CREATE OR REPLACE FUNCTION api.login(
  username varchar,
  password varchar,
  days     int DEFAULT 1
) RETURNS text
AS $$
DECLARE
  _user  record;
  result text;
BEGIN
  SELECT
    *
  FROM app.users
  WHERE
    app.users.username = login.username AND
    app.users.password = public.crypt(login.password, app.users.password)
  INTO _user;

  IF NOT FOUND THEN
    RAISE invalid_password USING message = 'invalid username or password';
  END IF;

  SELECT
    jwt.sign(
      row_to_json(r), settings.get('app.jwt_secret')
    ) AS token
  FROM (
    SELECT
      'user'                                        AS role,
      _user.dataset_id::varchar                     AS dataset,
      _user.id::varchar                             AS sub,
      extract(EPOCH FROM now())::int + days*24*3600 AS exp
  ) r
  INTO result;

  RETURN result;
END;
$$ SECURITY DEFINER LANGUAGE plpgsql;

CREATE FUNCTION app.eval(
  _expr text
) RETURNS void AS $$
BEGIN
  EXECUTE _expr;
END;
$$ LANGUAGE plpgsql;

CREATE FUNCTION app.invoke_sql_or_enqueue_job(
  _script    text,
  _type      varchar,
  _job_class varchar,
  _pipeline  app.pipelines
) RETURNS void AS $$
BEGIN
  IF _script IS NOT NULL THEN
    IF _type = 'sql' THEN
      EXECUTE _script USING _pipeline;
    ELSE
      INSERT INTO public.que_jobs (job_class, args)
      VALUES (_job_class, array_to_json(ARRAY[_pipeline.id]));
    END IF;
  END IF;
END;
$$ LANGUAGE plpgsql;

CREATE FUNCTION app.jsonb_merge(
  jsonb,
  jsonb
) RETURNS jsonb AS $$
  SELECT
    jsonb_strip_nulls( jsonb_object_agg(k,v) )
  FROM (
    SELECT
      COALESCE(first_json.key, second_json.key) k,
      CASE
      WHEN first_json.key  IS NULL THEN second_json.value
      WHEN second_json.key IS NULL THEN first_json.value
      ELSE
        CASE
        WHEN jsonb_typeof(first_json.value)  = 'object' AND
             jsonb_typeof(second_json.value) = 'object' THEN
          app.jsonb_merge(first_json.value, second_json.value)
        ELSE
          second_json.value
        END
      END AS v
    FROM            jsonb_each($1) AS first_json
    FULL OUTER JOIN jsonb_each($2) AS second_json ON
      first_json.key = second_json.key
  ) AS pairs( k, v );
$$ STABLE LANGUAGE sql;

CREATE FUNCTION app.pipe_process_enqueue_call(
  _pipeline     app.pipelines,
  _pipe_process app.pipe_processes
) RETURNS integer AS $$
  INSERT INTO public.que_jobs (
    job_class,
    args
  ) VALUES (
    'PipeProcess::CallJob',
    jsonb_build_array(
      _pipe_process.id::varchar,
      jsonb_build_object(
        'pipeline_id',           _pipeline.id,
        'pipeline_execution_id', _pipeline.pipeline_execution_id
      )
    )
  )
  RETURNING 1;
$$ LANGUAGE sql;

CREATE FUNCTION app.pipe_process_enqueue_call(
  _pipe_process app.pipe_processes
) RETURNS integer AS $$
  SELECT
    SUM( app.pipe_process_enqueue_call(pipeline, _pipe_process) )::integer
  FROM app.pipelines AS pipeline
  WHERE
    id = _pipe_process.pipeline_id;
$$ LANGUAGE sql;

CREATE FUNCTION app.update_pipeline_counters(
  uuid
) RETURNS void AS $$
  UPDATE app.pipelines
  SET total_count     = pipe_processes_query.total_count,
      succeeded_count = pipe_processes_query.succeeded_count + pipe_processes_query.skipped_count,
      failed_count    = pipe_processes_query.failed_count,
      waiting_count   = pipe_processes_query.waiting_count
  FROM (
    SELECT
      COUNT(*)                                                   AS total_count,
      COUNT(CASE WHEN status = 'succeeded' THEN 1 ELSE NULL END) AS succeeded_count,
      COUNT(CASE WHEN status = 'skipped'   THEN 1 ELSE NULL END) AS skipped_count,
      COUNT(CASE WHEN status = 'failed'    THEN 1 ELSE NULL END) AS failed_count,
      COUNT(CASE WHEN status = 'waiting'   THEN 1 ELSE NULL END) AS waiting_count
    FROM app.pipe_processes
    WHERE
      pipeline_id = $1
  ) AS pipe_processes_query
  WHERE app.pipelines.id = $1;
$$ SECURITY DEFINER LANGUAGE sql;

CREATE FUNCTION app.pipeline_call(
  uuid
) RETURNS void AS $$
  WITH called_pipelines AS (
    SELECT *
    FROM app.pipelines
    WHERE id = $1
  )

  SELECT SUM( app.pipe_process_enqueue_call(pipeline, pipe_process) )
  FROM
    app.pipe_processes AS pipe_process,
    called_pipelines   AS pipeline
  WHERE
    pipeline_id = $1;

  SELECT app.update_pipeline_counters($1);
$$ LANGUAGE sql;

CREATE FUNCTION app.pipeline_call(
  uuid,
  varchar
) RETURNS void AS $$
  WITH called_pipelines AS (
    SELECT *
    FROM app.pipelines
    WHERE id = $1
  )

  SELECT SUM( app.pipe_process_enqueue_call(pipeline, pipe_process) )
  FROM
    app.pipe_processes AS pipe_process,
    called_pipelines   AS pipeline
  WHERE
    pipeline_id         = $1 AND
    pipe_process.status = $2::app.pipe_process_status;

  SELECT app.update_pipeline_counters($1);
$$ LANGUAGE sql;

CREATE FUNCTION app.pipeline_call(
  uuid,
  varchar,
  integer
) RETURNS void AS $$
  WITH called_pipelines AS (
    SELECT *
    FROM app.pipelines
    WHERE id = $1
  )

  SELECT SUM( app.pipe_process_enqueue_call(pipeline, pipe_process) )
  FROM
    app.pipe_processes AS pipe_process,
    called_pipelines   AS pipeline
  WHERE
    pipeline_id         = $1 AND
    pipe_process.status = $2::app.pipe_process_status AND
    process_index       = $3;

  SELECT app.update_pipeline_counters($1);
$$ LANGUAGE sql;

CREATE FUNCTION app.pipeline_execution_invoke_call(
  uuid
) RETURNS void AS $$
  WITH pipeline_execution AS (
    UPDATE app.pipeline_executions
    SET run_at = NOW()
    WHERE id = $1
    RETURNING *
  )

  INSERT INTO app.pipelines (
    dataset_id,
    pipeline_template_id,
    pipeline_execution_id
  ) SELECT
    pipeline_execution.dataset_id,
    pipeline_execution.pipeline_template_id,
    pipeline_execution.id
  FROM pipeline_execution;

$$ LANGUAGE sql;

CREATE FUNCTION app.pipeline_execution_update_status(
  uuid
) RETURNS void AS $$
  WITH counters AS (
    SELECT
      COUNT(*) FILTER ( WHERE status = 'pending'   ) AS pending_count,
      COUNT(*) FILTER ( WHERE status = 'waiting'   ) AS waiting_count,
      COUNT(*) FILTER ( WHERE status = 'failed'    ) AS failed_count,
      COUNT(*) FILTER ( WHERE status = 'succeeded' ) AS succeeded_count,
      COUNT(*)                                       AS total_count
    FROM app.pipelines
    WHERE pipeline_execution_id = $1
  )

  UPDATE app.pipeline_executions
  SET status = CASE
    WHEN total_count = succeeded_count THEN 'succeeded'
    WHEN failed_count > 0              THEN 'failed'
    WHEN waiting_count > 0             THEN 'waiting'
    ELSE 'pending'
  END::app.pipeline_status
  FROM counters
  WHERE id = $1;
$$ LANGUAGE sql;

CREATE FUNCTION jwt.url_encode(data bytea) RETURNS text
AS $$
  SELECT translate(encode(data, 'base64'), E'+/=\n', '-_');
$$ IMMUTABLE LANGUAGE sql;

CREATE FUNCTION jwt.algorithm_sign(
  signables text,
  secret    text,
  algorithm text
) RETURNS text
AS $$
  WITH
    alg AS ( SELECT
      CASE
      WHEN algorithm = 'HS256' THEN 'sha256'
      WHEN algorithm = 'HS384' THEN 'sha384'
      WHEN algorithm = 'HS512' THEN 'sha512'
      ELSE ''
      END
    )  -- hmac throws error
  SELECT jwt.url_encode(public.hmac(signables, secret, (select * FROM alg)));
$$ LANGUAGE sql;

CREATE FUNCTION jwt.sign(
  payload json,
  secret text,
  algorithm text DEFAULT 'HS256'
) RETURNS text
AS $$
  WITH
    header AS (
      SELECT jwt.url_encode(convert_to('{"alg":"' || algorithm || '","typ":"JWT"}', 'utf8'))
    ),
    payload AS (
      SELECT jwt.url_encode(convert_to(payload::text, 'utf8'))
    ),
    signables AS (
      SELECT (SELECT * FROM header) || '.' || (SELECT * FROM payload)
    )

  SELECT
    (SELECT * FROM signables) ||
    '.' ||
    jwt.algorithm_sign(
      (SELECT * FROM signables),
      secret,
      algorithm
    );
$$ LANGUAGE sql;

CREATE FUNCTION jwt.url_decode(data text) RETURNS bytea
AS $$
WITH
  t   AS ( SELECT translate(data, '-_', '+/') ),
  rem AS ( SELECT length((SELECT * FROM t)) % 4) -- compute padding size
  SELECT decode(
    (SELECT * FROM t) ||
    CASE WHEN (SELECT * FROM rem) > 0
      THEN repeat('=', (4 - (SELECT * FROM rem)))
      ELSE ''
    END,
    'base64'
  );
$$ IMMUTABLE LANGUAGE sql;

CREATE FUNCTION jwt.verify(
  token     text,
  secret    text,
  algorithm text DEFAULT 'HS256'
) RETURNS table(header json, payload json, valid boolean)
AS $$
  SELECT
    convert_from(jwt.url_decode(r[1]), 'utf8')::json AS header,
    convert_from(jwt.url_decode(r[2]), 'utf8')::json AS payload,
    r[3] = jwt.algorithm_sign(r[1] || '.' || r[2], secret, algorithm) AS valid
  FROM regexp_split_to_array(token, '\.') r;
$$ STABLE LANGUAGE sql;

CREATE OR REPLACE FUNCTION public.if_dataset_id(id uuid, value anyelement) RETURNS anyelement AS $$
BEGIN
  IF current_setting('request.jwt.claim.dataset', true)::uuid = id THEN
    RETURN value;
  ELSE
    RETURN NULL;
  END IF;
END;
$$ STABLE LANGUAGE plpgsql;

CREATE OR REPLACE FUNCTION public.if_user_by_id(id uuid, value anyelement) RETURNS anyelement AS $$
BEGIN
  IF current_user = 'user' AND current_setting('request.jwt.claim.sub', true)::uuid = id THEN
    RETURN value;
  ELSE
    RETURN NULL;
  END IF;
END;
$$ STABLE LANGUAGE plpgsql;

CREATE FUNCTION public.que_determine_job_state(job public.que_jobs) RETURNS text
    LANGUAGE sql
    AS $$
  SELECT
    CASE
    WHEN job.expired_at  IS NOT NULL    THEN 'expired'
    WHEN job.finished_at IS NOT NULL    THEN 'finished'
    WHEN job.error_count > 0            THEN 'errored'
    WHEN job.run_at > CURRENT_TIMESTAMP THEN 'scheduled'
    ELSE                                     'ready'
    END
$$;

CREATE FUNCTION public.que_job_notify() RETURNS trigger
    LANGUAGE plpgsql
    AS $$
  DECLARE
    locker_pid integer;
    sort_key json;
  BEGIN
    -- Don't do anything if the job is scheduled for a future time.
    IF NEW.run_at IS NOT NULL AND NEW.run_at > NOW() THEN
      RETURN null;
    END IF;

    -- Pick a locker to notify of the job's insertion, weighted by their number
    -- of workers. Should bounce pseudorandomly between lockers on each
    -- invocation, hence the md5-ordering, but still touch each one equally,
    -- hence the modulo using the job_id.
    SELECT pid
    INTO locker_pid
    FROM (
      SELECT *, last_value(row_number) OVER () + 1 AS count
      FROM (
        SELECT *, row_number() OVER () - 1 AS row_number
        FROM (
          SELECT *
          FROM public.que_lockers ql, generate_series(1, ql.worker_count) AS id
          WHERE listening AND queues @> ARRAY[NEW.queue]
          ORDER BY md5(pid::text || id::text)
        ) t1
      ) t2
    ) t3
    WHERE NEW.id % count = row_number;

    IF locker_pid IS NOT NULL THEN
      -- There's a size limit to what can be broadcast via LISTEN/NOTIFY, so
      -- rather than throw errors when someone enqueues a big job, just
      -- broadcast the most pertinent information, and let the locker query for
      -- the record after it's taken the lock. The worker will have to hit the
      -- DB in order to make sure the job is still visible anyway.
      SELECT row_to_json(t)
      INTO sort_key
      FROM (
        SELECT
          'job_available' AS message_type,
          NEW.queue       AS queue,
          NEW.priority    AS priority,
          NEW.id          AS id,
          -- Make sure we output timestamps as UTC ISO 8601
          to_char(NEW.run_at AT TIME ZONE 'UTC', 'YYYY-MM-DD"T"HH24:MI:SS.US"Z"') AS run_at
      ) t;

      PERFORM pg_notify('que_listener_' || locker_pid::text, sort_key::text);
    END IF;

    RETURN null;
  END
$$;

CREATE FUNCTION public.que_state_notify() RETURNS trigger
    LANGUAGE plpgsql
    AS $$
  DECLARE
    row record;
    message json;
    previous_state text;
    current_state text;
  BEGIN
    IF TG_OP = 'INSERT' THEN
      previous_state := 'nonexistent';
      current_state  := public.que_determine_job_state(NEW);
      row            := NEW;
    ELSIF TG_OP = 'DELETE' THEN
      previous_state := public.que_determine_job_state(OLD);
      current_state  := 'nonexistent';
      row            := OLD;
    ELSIF TG_OP = 'UPDATE' THEN
      previous_state := public.que_determine_job_state(OLD);
      current_state  := public.que_determine_job_state(NEW);

      -- If the state didn't change, short-circuit.
      IF previous_state = current_state THEN
        RETURN null;
      END IF;

      row := NEW;
    ELSE
      RAISE EXCEPTION 'Unrecognized TG_OP: %', TG_OP;
    END IF;

    SELECT row_to_json(t)
    INTO message
    FROM (
      SELECT
        'job_change' AS message_type,
        row.id       AS id,
        row.queue    AS queue,

        coalesce(row.data->'tags', '[]'::jsonb) AS tags,

        to_char(row.run_at AT TIME ZONE 'UTC', 'YYYY-MM-DD"T"HH24:MI:SS.US"Z"') AS run_at,
        to_char(NOW()      AT TIME ZONE 'UTC', 'YYYY-MM-DD"T"HH24:MI:SS.US"Z"') AS time,

        CASE row.job_class
        WHEN 'ActiveJob::QueueAdapters::QueAdapter::JobWrapper' THEN
          coalesce(
            row.args->0->>'job_class',
            'ActiveJob::QueueAdapters::QueAdapter::JobWrapper'
          )
        ELSE
          row.job_class
        END AS job_class,

        previous_state AS previous_state,
        current_state  AS current_state
    ) t;

    PERFORM pg_notify('que_state', message::text);

    RETURN null;
  END
$$;

CREATE FUNCTION settings.get(varchar)
RETURNS varchar
AS $$
  SELECT value
  FROM settings.secrets
  WHERE key = $1
$$ SECURITY DEFINER STABLE LANGUAGE sql;

CREATE FUNCTION settings.set(varchar, varchar)
RETURNS void
AS $$
  INSERT INTO settings.secrets (key, value)
  VALUES ($1, $2)
  ON CONFLICT (key) DO UPDATE
  SET value = $2;
$$ SECURITY DEFINER LANGUAGE sql;

CREATE FUNCTION triggers.pipe_process_delete_que_jobs()
RETURNS trigger AS $$
BEGIN
  DELETE FROM public.que_jobs
  WHERE
    job_class IN ('PipeProcess::CallJob', 'PipeProcess::RetryJob')
    AND args->>0 = OLD.id::varchar;

  RETURN OLD;
END;
$$ SECURITY DEFINER LANGUAGE plpgsql;

CREATE FUNCTION triggers.pipe_process_update_pipeline_counters()
RETURNS trigger AS $$
DECLARE
  waiting_increment   int;
  failed_increment    int;
  succeeded_increment int;
BEGIN
  IF ( OLD.status = NEW.status                               ) OR
     ( OLD.status = 'skipped'   AND NEW.status = 'succeeded' ) OR
     ( OLD.status = 'succeeded' AND NEW.status = 'skipped'   ) THEN
    RETURN NEW;
  END IF;

  waiting_increment   = 0;
  failed_increment    = 0;
  succeeded_increment = 0;

  CASE OLD.status
  WHEN 'waiting' THEN
    waiting_increment = -1;
  WHEN 'failed' THEN
    failed_increment = -1;
  WHEN 'skipped' THEN
    succeeded_increment = -1;
  WHEN 'succeeded' THEN
    succeeded_increment = -1;
  ELSE
  END CASE;

  CASE NEW.status
  WHEN 'waiting' THEN
    waiting_increment = waiting_increment + 1;
  WHEN 'failed' THEN
    failed_increment = failed_increment + 1;
  WHEN 'skipped' THEN
    succeeded_increment = succeeded_increment + 1;
  WHEN 'succeeded' THEN
    succeeded_increment = succeeded_increment + 1;
  ELSE
  END CASE;

  UPDATE app.pipelines
  SET
    waiting_count   = waiting_count   + waiting_increment,
    failed_count    = failed_count    + failed_increment,
    succeeded_count = succeeded_count + succeeded_increment
  WHERE
    id = NEW.pipeline_id;

  RETURN NEW;
END;
$$ SECURITY DEFINER LANGUAGE plpgsql;

CREATE FUNCTION triggers.pipe_process_update_pipeline_total_count()
RETURNS trigger AS $$
BEGIN
  UPDATE app.pipelines
  SET
    total_count = total_count + 1
  WHERE
    id = NEW.pipeline_id;

  RETURN NEW;
END;
$$ SECURITY DEFINER LANGUAGE plpgsql;

CREATE FUNCTION triggers.pipeline_delete_que_jobs()
RETURNS trigger AS $$
BEGIN
  DELETE FROM public.que_jobs
  WHERE
    job_class IN ('Pipeline::BootstrapJob', 'Pipeline::WaitingJob', 'Pipeline::SuccessJob', 'Pipeline::FailJob', 'Pipeline::NotifyJob')
    AND args->>0 = OLD.id::varchar;

  RETURN OLD;
END;
$$ SECURITY DEFINER LANGUAGE plpgsql;

CREATE FUNCTION triggers.pipeline_execution_delete_que_jobs()
RETURNS trigger AS $$
BEGIN
  DELETE FROM public.que_jobs
  WHERE
    job_class IN ('PipelineExecution::CallJob')
    AND args->>0 = OLD.id::varchar;

  RETURN OLD;
END;
$$ SECURITY DEFINER LANGUAGE plpgsql;

CREATE FUNCTION triggers.pipeline_execution_schedule_itself()
RETURNS trigger AS $$
BEGIN
  INSERT INTO public.que_jobs (
    job_class,
    args,
    run_at
  ) VALUES (
    'PipelineExecution::CallJob',
    jsonb_build_array(NEW.id::varchar),
    COALESCE(
      NEW.run_at,
      NOW() + COALESCE(NEW.schedule_interval, '0 seconds'::interval)
    )
  );

  RETURN NEW;
END;
$$ SECURITY DEFINER LANGUAGE plpgsql;

CREATE FUNCTION triggers.pipeline_execution_schedule_next()
RETURNS trigger AS $$
BEGIN
  IF OLD.status != NEW.status AND NEW.status = 'succeeded' AND NEW.schedule_interval IS NOT NULL THEN
    INSERT INTO app.pipeline_executions (
      dataset_id,
      pipeline_template_id,
      schedule_interval,
      counter,
      name
    ) VALUES (
      NEW.dataset_id,
      NEW.pipeline_template_id,
      NEW.schedule_interval,
      NEW.counter + 1,
      NEW.name
    );
  END IF;

  RETURN NEW;
END;
$$ SECURITY DEFINER LANGUAGE plpgsql;

CREATE FUNCTION triggers.pipeline_invoke_bootstrap()
RETURNS trigger AS $$
BEGIN
  PERFORM app.invoke_sql_or_enqueue_job(
    NEW.bootstrap_script,
    NEW.bootstrap_script_type,
    'Pipeline::BootstrapJob',
    NEW
  );

  RETURN NEW;
END;
$$ SECURITY DEFINER LANGUAGE plpgsql;

CREATE FUNCTION triggers.pipeline_invoke_status_callback()
RETURNS trigger AS $$
BEGIN
  IF OLD.status = NEW.status THEN
    RETURN NEW;
  END IF;

  CASE NEW.status
  WHEN 'succeeded' THEN
    CASE WHEN NEW.success_script_type IS NULL THEN
      INSERT INTO public.que_jobs (job_class, args)
      VALUES ('Pipeline::NotifyJob', array_to_json(ARRAY[NEW.id]));
    ELSE
      PERFORM app.invoke_sql_or_enqueue_job(
        NEW.success_script,
        NEW.success_script_type,
        'Pipeline::SuccessJob',
        NEW
      );
    END CASE;

  WHEN 'failed' THEN
    CASE WHEN NEW.fail_script_type IS NULL THEN
      INSERT INTO public.que_jobs (job_class, args)
      VALUES ('Pipeline::NotifyJob', array_to_json(ARRAY[NEW.id]));
    ELSE
      PERFORM app.invoke_sql_or_enqueue_job(
        NEW.fail_script,
        NEW.fail_script_type,
        'Pipeline::FailJob',
        NEW
      );
    END CASE;
  WHEN 'waiting' THEN
    CASE WHEN NEW.waiting_script_type IS NULL THEN
      INSERT INTO public.que_jobs (job_class, args)
      VALUES ('Pipeline::NotifyJob', array_to_json(ARRAY[NEW.id]));
    ELSE
      PERFORM app.invoke_sql_or_enqueue_job(
        NEW.waiting_script,
        NEW.waiting_script_type,
        'Pipeline::WaitingJob',
        NEW
      );
    END CASE;
  ELSE
  END CASE;

  IF NEW.status != OLD.status THEN
    PERFORM app.pipeline_execution_update_status(NEW.pipeline_execution_id);
  END IF;

  RETURN NEW;
END;
$$ SECURITY DEFINER LANGUAGE plpgsql;

CREATE FUNCTION triggers.pipeline_setup_from_template()
RETURNS trigger AS $$
DECLARE
  template record;
BEGIN
  SELECT *
  FROM app.pipeline_templates
  WHERE id = NEW.pipeline_template_id
  INTO template;

  IF NEW.dataset_id IS NULL THEN
    NEW.dataset_id = template.dataset_id;
  END IF;

  IF NEW.max_retries IS NULL THEN
    NEW.max_retries = template.max_retries;
  END IF;

  IF NEW.name IS NULL THEN
    NEW.name = template.name;
  END IF;

  IF NEW.data IS NULL THEN
    NEW.data = template.data;
  END IF;

  IF NEW.pipes IS NULL THEN
    NEW.pipes = template.pipes;
  END IF;

  IF NEW.bootstrap_script_type IS NULL THEN
    NEW.bootstrap_script_type = template.bootstrap_script_type;
  END IF;

  IF NEW.success_script_type IS NULL THEN
    NEW.success_script_type = template.success_script_type;
  END IF;

  IF NEW.fail_script_type IS NULL THEN
    NEW.fail_script_type = template.fail_script_type;
  END IF;

  IF NEW.waiting_script_type IS NULL THEN
    NEW.waiting_script_type = template.waiting_script_type;
  END IF;

  IF NEW.bootstrap_script IS NULL THEN
    NEW.bootstrap_script = template.bootstrap_script;
  END IF;

  IF NEW.success_script IS NULL THEN
    NEW.success_script = template.success_script;
  END IF;

  IF NEW.fail_script IS NULL THEN
    NEW.fail_script = template.fail_script;
  END IF;

  IF NEW.waiting_script IS NULL THEN
    NEW.waiting_script = template.waiting_script;
  END IF;

  RETURN NEW;
END;
$$ SECURITY DEFINER LANGUAGE plpgsql;

CREATE FUNCTION triggers.pipeline_update_status()
RETURNS trigger AS $$
BEGIN
  IF NEW.total_count <= 0 OR (NEW.succeeded_count + NEW.failed_count + NEW.waiting_count != NEW.total_count) THEN
    NEW.status = 'pending';
  ELSE
    IF NEW.waiting_count > 0 THEN
      NEW.status = 'waiting';
    END IF;

    IF NEW.failed_count > 0 THEN
      NEW.status = 'failed';
    END IF;

    IF NEW.succeeded_count = NEW.total_count THEN
      NEW.status = 'succeeded';
    END IF;
  END IF;

  RETURN NEW;
END;
$$ SECURITY DEFINER LANGUAGE plpgsql;

CREATE FUNCTION triggers.resource_increase_sequence()
RETURNS trigger AS $$
BEGIN
  IF NEW.sequence = OLD.sequence THEN
    NEW.sequence = OLD.sequence + 1;
  END IF;
  RETURN NEW;
END;
$$ SECURITY DEFINER LANGUAGE plpgsql;

CREATE FUNCTION triggers.resource_keeps_version()
RETURNS trigger AS $$
DECLARE
  new_dataset_sequence bigint;
BEGIN
  UPDATE app.datasets
  SET sequence = sequence + 1
  WHERE
    id = NEW.dataset_id
  RETURNING sequence
  INTO new_dataset_sequence;

  INSERT INTO app.resource_versions
    (
      dataset_sequence,
      sequence,
      resource_id,
      kind,
      schema_version,
      status,
      unique_id,
      content,
      relations,
      dataset_id,
      extra,
      last_execution_id,
      created_at,
      updated_at
    )
  VALUES
    (
      new_dataset_sequence,
      NEW.sequence,
      NEW.id,
      NEW.kind,
      NEW.schema_version,
      NEW.status,
      NEW.unique_id,
      NEW.content,
      NEW.relations,
      NEW.dataset_id,
      NEW.extra,
      NEW.last_execution_id,
      NEW.created_at,
      NEW.updated_at
    );

  RETURN NEW;
END;
$$ SECURITY DEFINER LANGUAGE plpgsql;

CREATE FUNCTION triggers.resource_validate_content()
RETURNS trigger AS $$
DECLARE
  is_valid boolean;
BEGIN
  IF NEW.schema_version IS NULL OR NEW.status != 'active' THEN
    RETURN NEW;
  END IF;

  SELECT
    json_schema.validate_json_schema(resource_schema.specification, NEW.content)
  FROM app.resource_schemas AS resource_schema
  WHERE
    kind           = NEW.kind AND
    schema_version = NEW.schema_version
  LIMIT 1
  INTO is_valid;

  IF NOT FOUND OR NOT is_valid THEN
  -- TODO: Handle resource schema errors with proper error message
    RAISE EXCEPTION 'Invalid content for resource' USING HINT = 'Invalid content for resource';
  END IF;

  RETURN NEW;
END;
$$ SECURITY DEFINER LANGUAGE plpgsql;

CREATE FUNCTION triggers.track_updated_at() RETURNS trigger
AS $$
BEGIN
  IF NEW.updated_at IS NULL THEN
    NEW.updated_at := OLD.updated_at;
  ELSE
    IF NEW.updated_at = OLD.updated_at THEN
      NEW.updated_at := NOW();
    END IF;
  END IF;

  RETURN NEW;
END;
$$ SECURITY DEFINER LANGUAGE plpgsql;

CREATE FUNCTION triggers.user_encrypt_password()
RETURNS trigger AS $$
BEGIN
  IF NEW.password ~* '^\$\d\w\$\d{2}\$[^$]{53}$' THEN
    RETURN NEW;
  END IF;

  IF tg_op = 'INSERT' OR NEW.password != OLD.password THEN
    NEW.password = crypt(NEW.password, gen_salt('bf', 11));
  END IF;

  RETURN NEW;
end
$$ SECURITY DEFINER LANGUAGE plpgsql;

CREATE FUNCTION util.pipe_process_retry_call(
  _id             uuid,
  _process_index  integer DEFAULT 0,
  _accumulator    jsonb   DEFAULT NULL,
  _data           jsonb   DEFAULT NULL
) RETURNS bigint AS $$
  WITH updated_processes AS (
    UPDATE app.pipe_processes
    SET
      status           = 'pending',
      process_index    = _process_index,
      data             = _data,
      accumulator      = _accumulator,
      last_data        = NULL,
      last_accumulator = NULL,
      error_backtrace  = NULL
    WHERE id = _id
    RETURNING *
  )

  SELECT SUM( app.pipe_process_enqueue_call(pipe_process) )
  FROM updated_processes AS pipe_process;

$$ LANGUAGE sql;

CREATE FUNCTION util.resource_versions_prune()
RETURNS void AS $$
  DELETE FROM app.resource_versions AS version
  WHERE EXISTS (
    SELECT *
    FROM app.resource_versions AS latest_version
    WHERE
      version.resource_id = latest_version.resource_id AND
      version.id          < latest_version.id
  )
$$ LANGUAGE sql;

CREATE TRIGGER track_updated_at
  BEFORE UPDATE
  ON app.datasets
  FOR EACH ROW
    EXECUTE PROCEDURE triggers.track_updated_at();

CREATE TRIGGER pipe_process_update_pipeline_total_count
  AFTER INSERT
  ON app.pipe_processes
  FOR EACH ROW
    EXECUTE PROCEDURE triggers.pipe_process_update_pipeline_total_count();

CREATE TRIGGER pipe_process_update_pipeline_counters
  AFTER UPDATE
  ON app.pipe_processes
  FOR EACH ROW
    EXECUTE PROCEDURE triggers.pipe_process_update_pipeline_counters();

CREATE TRIGGER track_updated_at
  BEFORE UPDATE
  ON app.pipe_processes
  FOR EACH ROW
    EXECUTE PROCEDURE triggers.track_updated_at();

CREATE TRIGGER pipe_process_delete_que_jobs
  AFTER DELETE
  ON app.pipe_processes
  FOR EACH ROW
    EXECUTE PROCEDURE triggers.pipe_process_delete_que_jobs();

CREATE TRIGGER pipeline_execution_schedule_itself
  AFTER INSERT
  ON app.pipeline_executions
  FOR EACH ROW
    EXECUTE PROCEDURE triggers.pipeline_execution_schedule_itself();

CREATE TRIGGER track_updated_at
  BEFORE UPDATE
  ON app.pipeline_executions
  FOR EACH ROW
    EXECUTE PROCEDURE triggers.track_updated_at();

CREATE TRIGGER pipeline_execution_schedule_next
  AFTER UPDATE
  ON app.pipeline_executions
  FOR EACH ROW
    EXECUTE PROCEDURE triggers.pipeline_execution_schedule_next();

CREATE TRIGGER pipeline_execution_delete_que_jobs
  AFTER DELETE
  ON app.pipeline_executions
  FOR EACH ROW
    EXECUTE PROCEDURE triggers.pipeline_execution_delete_que_jobs();

CREATE TRIGGER track_updated_at
  BEFORE UPDATE
  ON app.pipeline_templates
  FOR EACH ROW
    EXECUTE PROCEDURE triggers.track_updated_at();

CREATE TRIGGER pipeline_setup_from_template
  BEFORE INSERT
  ON app.pipelines
  FOR EACH ROW
    EXECUTE PROCEDURE triggers.pipeline_setup_from_template();

CREATE TRIGGER pipeline_invoke_bootstrap
  AFTER INSERT
  ON app.pipelines
  FOR EACH ROW
    EXECUTE PROCEDURE triggers.pipeline_invoke_bootstrap();

CREATE TRIGGER pipeline_update_status
  BEFORE UPDATE
  ON app.pipelines
  FOR EACH ROW
    EXECUTE PROCEDURE triggers.pipeline_update_status();

CREATE TRIGGER track_updated_at
  BEFORE UPDATE
  ON app.pipelines
  FOR EACH ROW
    EXECUTE PROCEDURE triggers.track_updated_at();

CREATE TRIGGER pipeline_invoke_status_callback
  AFTER UPDATE
  ON app.pipelines
  FOR EACH ROW
    EXECUTE PROCEDURE triggers.pipeline_invoke_status_callback();

CREATE TRIGGER pipeline_delete_que_jobs
  AFTER DELETE
  ON app.pipelines
  FOR EACH ROW
    EXECUTE PROCEDURE triggers.pipeline_delete_que_jobs();

CREATE TRIGGER track_updated_at
  BEFORE UPDATE
  ON app.resource_schemas
  FOR EACH ROW
    EXECUTE PROCEDURE triggers.track_updated_at();

CREATE TRIGGER resource_increase_sequence
  BEFORE UPDATE
  ON app.resources
  FOR EACH ROW
    EXECUTE PROCEDURE triggers.resource_increase_sequence();

CREATE TRIGGER track_updated_at
  BEFORE UPDATE
  ON app.resources
  FOR EACH ROW
    EXECUTE PROCEDURE triggers.track_updated_at();

CREATE CONSTRAINT TRIGGER resource_validate_content
  AFTER INSERT OR UPDATE
  ON app.resources
  FOR EACH ROW
    EXECUTE PROCEDURE triggers.resource_validate_content();

CREATE TRIGGER resource_keeps_version
  AFTER INSERT OR UPDATE
  ON app.resources
  FOR EACH ROW
    EXECUTE PROCEDURE triggers.resource_keeps_version();

CREATE TRIGGER user_encrypt_password
  BEFORE INSERT OR UPDATE
  ON app.users
  FOR EACH ROW
    EXECUTE PROCEDURE triggers.user_encrypt_password();

CREATE TRIGGER track_updated_at
  BEFORE UPDATE
  ON app.users
  FOR EACH ROW
    EXECUTE PROCEDURE triggers.track_updated_at();

CREATE TRIGGER track_updated_at
  BEFORE UPDATE
  ON public.ar_internal_metadata
  FOR EACH ROW
    EXECUTE PROCEDURE triggers.track_updated_at();

CREATE TRIGGER que_job_notify
  AFTER INSERT
  ON public.que_jobs
  FOR EACH ROW
    EXECUTE PROCEDURE public.que_job_notify();

CREATE TRIGGER que_state_notify
  AFTER INSERT OR DELETE OR UPDATE
  ON public.que_jobs
  FOR EACH ROW
    EXECUTE PROCEDURE public.que_state_notify();

CREATE INDEX index_pipe_processes_on_pipeline_id
ON app.pipe_processes
USING btree (pipeline_id);

CREATE INDEX index_pipe_processes_on_status
ON app.pipe_processes
USING btree (status);

CREATE INDEX index_pipe_processes_on_pipeline_id_and_status
ON app.pipe_processes
USING btree (pipeline_id, status);

CREATE INDEX index_pipeline_templates_on_dataset_id
ON app.pipeline_templates
USING btree (dataset_id);

CREATE INDEX index_pipelines_on_dataset_id
ON app.pipelines
USING btree (dataset_id);

CREATE INDEX index_pipelines_on_pipeline_template_id
ON app.pipelines
USING btree (pipeline_template_id);

CREATE INDEX index_pipelines_on_status
ON app.pipelines
USING btree (status);

CREATE INDEX index_pipelines_on_pipeline_execution_id_and_status
ON app.pipelines
USING btree (pipeline_execution_id, status);

CREATE INDEX index_resource_versions_on_dataset_id
ON app.resource_versions
USING btree (dataset_id);

CREATE UNIQUE INDEX index_resource_versions_on_dataset_id_and_dataset_sequence
ON app.resource_versions
USING btree (dataset_id, dataset_sequence);

CREATE INDEX index_resource_versions_on_dataset_id_and_kind_and_unique_id
ON app.resource_versions
USING btree (dataset_id, kind, unique_id);

CREATE INDEX index_resource_versions_on_resource_id
ON app.resource_versions
USING btree (resource_id);

CREATE UNIQUE INDEX index_resource_versions_on_resource_id_and_sequence
ON app.resource_versions
USING btree (resource_id, sequence);

CREATE INDEX index_resources_on_dataset_id
ON app.resources
USING btree (dataset_id);

CREATE INDEX index_resources_on_dataset_id_and_kind
ON app.resources
USING btree (dataset_id, kind);

CREATE UNIQUE INDEX index_resources_on_dataset_id_and_kind_and_unique_id
ON app.resources USING btree (dataset_id, kind, unique_id);

CREATE UNIQUE INDEX index_user_accounts_on_username
ON app.users
USING btree (username);

CREATE INDEX que_jobs_args_gin_idx
ON public.que_jobs
USING gin (args jsonb_path_ops);

CREATE INDEX que_jobs_data_gin_idx
ON public.que_jobs
USING gin (data jsonb_path_ops);

CREATE INDEX que_poll_idx
ON public.que_jobs
USING btree (queue, priority, run_at, id)
WHERE ((finished_at IS NULL) AND (expired_at IS NULL));

CREATE OR REPLACE VIEW api.pipeline_executions AS
  SELECT
    id,
    dataset_id,
    status,
    pipeline_template_id,
    schedule_interval,
    created_at,
    updated_at,
    run_at,
    counter,
    name
  FROM app.pipeline_executions
  WHERE
    if_dataset_id(dataset_id, TRUE);

CREATE OR REPLACE FUNCTION triggers.api_pipeline_executions_instead_insert() RETURNS trigger AS $$
DECLARE
  entry RECORD;
BEGIN
  INSERT INTO app.pipeline_executions (
    status,
    pipeline_template_id,
    schedule_interval,
    run_at,
    counter,
    name,
    dataset_id
  ) VALUES (
    COALESCE(NEW.status, 'pending'),
    NEW.pipeline_template_id,
    NEW.schedule_interval,
    NEW.run_at,
    COALESCE(NEW.counter, 1),
    NEW.name,
    current_setting('request.jwt.claim.dataset', true)::uuid
  ) RETURNING
    id,
    dataset_id,
    status,
    pipeline_template_id,
    schedule_interval,
    created_at,
    updated_at,
    run_at,
    counter,
    name
  INTO entry;

  RETURN entry;
END;
$$ LANGUAGE plpgsql;

CREATE TRIGGER api_pipeline_executions_instead_insert
  INSTEAD OF INSERT
  ON api.pipeline_executions
  FOR EACH ROW
    EXECUTE PROCEDURE triggers.api_pipeline_executions_instead_insert();

CREATE OR REPLACE VIEW api.pipeline_templates AS
  SELECT
    id,
    created_at,
    updated_at,
    max_retries,
    name,
    data,
    pipes,
    bootstrap_script_type,
    success_script_type,
    fail_script_type,
    waiting_script_type,
    bootstrap_script,
    success_script,
    fail_script,
    waiting_script
  FROM app.pipeline_templates
  WHERE
    if_dataset_id(dataset_id, TRUE);

CREATE OR REPLACE FUNCTION triggers.api_pipeline_templates_instead_insert() RETURNS trigger AS $$
DECLARE
  entry RECORD;
BEGIN
  INSERT INTO app.pipeline_templates (
    max_retries,
    name,
    data,
    pipes,
    bootstrap_script_type,
    success_script_type,
    fail_script_type,
    waiting_script_type,
    bootstrap_script,
    success_script,
    fail_script,
    waiting_script,
    dataset_id
  ) VALUES (
    COALESCE(NEW.max_retries, 5),
    NEW.name,
    NEW.data,
    NEW.pipes,
    NEW.bootstrap_script_type,
    NEW.success_script_type,
    NEW.fail_script_type,
    NEW.waiting_script_type,
    NEW.bootstrap_script,
    NEW.success_script,
    NEW.fail_script,
    NEW.waiting_script,
    current_setting('request.jwt.claim.dataset', true)::uuid
  ) RETURNING
    id,
    created_at,
    updated_at,
    max_retries,
    name,
    data,
    pipes,
    bootstrap_script_type,
    success_script_type,
    fail_script_type,
    waiting_script_type,
    bootstrap_script,
    success_script,
    fail_script,
    waiting_script
  INTO entry;

  RETURN entry;
END;
$$ LANGUAGE plpgsql;

CREATE TRIGGER api_pipeline_templates_instead_insert
  INSTEAD OF INSERT
  ON api.pipeline_templates
  FOR EACH ROW
    EXECUTE PROCEDURE triggers.api_pipeline_templates_instead_insert();

CREATE OR REPLACE VIEW api.resource_schemas AS
  SELECT
    id,
    dataset_id,
    created_at,
    updated_at,
    kind,
    schema_version,
    specification,
    public_specification
  FROM app.resource_schemas
  WHERE
    if_dataset_id(dataset_id, TRUE);

CREATE OR REPLACE FUNCTION triggers.api_resource_schemas_instead_insert() RETURNS trigger AS $$
DECLARE
  entry RECORD;
BEGIN
  INSERT INTO app.resource_schemas (
    kind,
    schema_version,
    specification,
    public_specification,
    dataset_id
  ) VALUES (
    NEW.kind,
    NEW.schema_version,
    NEW.specification,
    NEW.public_specification,
    current_setting('request.jwt.claim.dataset', true)::uuid
  ) RETURNING
    id,
    dataset_id,
    created_at,
    updated_at,
    kind,
    schema_version,
    specification,
    public_specification
  INTO entry;

  RETURN entry;
END;
$$ LANGUAGE plpgsql;

CREATE TRIGGER api_resource_schemas_instead_insert
  INSTEAD OF INSERT
  ON api.resource_schemas
  FOR EACH ROW
    EXECUTE PROCEDURE triggers.api_resource_schemas_instead_insert();

CREATE OR REPLACE VIEW api.resource_versions AS
  SELECT
    id,
    resource_id,
    dataset_sequence,
    dataset_id,
    sequence,
    status,
    created_at,
    updated_at,
    unique_id,
    kind,
    schema_version,
    content,
    relations,
    extra,
    last_execution_id
  FROM app.resource_versions
  WHERE
    if_dataset_id(dataset_id, TRUE);

CREATE OR REPLACE VIEW api.resources AS
  SELECT
    id,
    dataset_id,
    sequence,
    status,
    created_at,
    updated_at,
    unique_id,
    kind,
    schema_version,
    content,
    relations,
    extra,
    last_execution_id
  FROM app.resources
  WHERE
    if_dataset_id(dataset_id, TRUE);

CREATE OR REPLACE VIEW api.users AS
  SELECT
    id,
    username,
    NULL::varchar AS password,
    dataset_id,
    created_at,
    updated_at
  FROM app.users
  WHERE
    if_user_by_id(id,         TRUE) AND
    if_dataset_id(dataset_id, TRUE);

CREATE ROLE "anonymous";

GRANT USAGE ON SCHEMA api      TO "anonymous";
GRANT USAGE ON SCHEMA jwt      TO "anonymous";
GRANT USAGE ON SCHEMA settings TO "anonymous";

CREATE ROLE "user";

GRANT USAGE ON SCHEMA api      TO "user";
GRANT USAGE ON SCHEMA app      TO "user";
GRANT USAGE ON SCHEMA jwt      TO "user";
GRANT USAGE ON SCHEMA settings TO "user";

GRANT ALL PRIVILEGES ON ALL SEQUENCES IN SCHEMA app TO "user";

GRANT SELECT( id, username,           dataset_id, created_at, updated_at ) ON app.users TO "user";
GRANT UPDATE( id, username, password                                     ) ON app.users TO "user";
GRANT SELECT( id, username, password, dataset_id, created_at, updated_at ) ON api.users TO "user";
GRANT UPDATE( id, username, password                                     ) ON api.users TO "user";

GRANT SELECT, INSERT, UPDATE, DELETE, REFERENCES ON TABLE app.pipeline_templates TO "user";
GRANT SELECT, INSERT,         DELETE, REFERENCES ON TABLE api.pipeline_templates TO "user";
GRANT UPDATE(
  name,
  data,
  pipes,
  max_retries,
  bootstrap_script_type,
  success_script_type,
  fail_script_type,
  waiting_script_type,
  bootstrap_script,
  success_script,
  fail_script,
  waiting_script
) ON api.pipeline_templates TO "user";

GRANT SELECT, INSERT, DELETE, REFERENCES ON TABLE app.pipeline_executions TO "user";
GRANT SELECT, INSERT, DELETE, REFERENCES ON TABLE api.pipeline_executions TO "user";

GRANT SELECT, INSERT, UPDATE, DELETE, REFERENCES ON TABLE app.resource_schemas TO "user";
GRANT SELECT, INSERT,         DELETE, REFERENCES ON TABLE api.resource_schemas TO "user";
GRANT UPDATE(
  kind,
  schema_version,
  specification
) ON api.resource_schemas TO "user";

GRANT SELECT ON TABLE app.resources TO "user";
GRANT SELECT ON TABLE api.resources TO "user";

GRANT SELECT ON TABLE app.resource_versions TO "user";
GRANT SELECT ON TABLE api.resource_versions TO "user";

CREATE ROLE "authenticator" NOINHERIT login password '$AUTHENTICATOR_PASSWORD';

GRANT "user"      TO "authenticator";
GRANT "anonymous" TO "authenticator";

GRANT USAGE ON SCHEMA api      TO "authenticator";
GRANT USAGE ON SCHEMA jwt      TO "authenticator";
GRANT USAGE ON SCHEMA settings TO "authenticator";

GRANT ALL PRIVILEGES ON ALL SEQUENCES IN SCHEMA app TO "authenticator";

CREATE USER grafana WITH PASSWORD '$GRAFANA_PASSWORD';

GRANT CONNECT ON DATABASE $POSTGRES_DB    TO "grafana";
GRANT USAGE   ON SCHEMA app               TO "grafana";
GRANT SELECT  ON ALL TABLES IN SCHEMA app TO "grafana";

INSERT INTO public.schema_migrations(version) VALUES
  ('20200120153357'),
  ('20200123173113'),
  ('20200211161612'),
  ('20200212123905'),
  ('20200623130306');

INSERT INTO settings.secrets (key, value) VALUES ('app.jwt_secret', '$JWT_SECRET');

COMMIT;
